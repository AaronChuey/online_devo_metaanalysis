
```{r result-setup}
library(tidyverse)
library(googledrive)
library(readxl)
library(metafor)
library(assertthat)
library(glue)
library(wesanderson)
library(gridExtra)
library(ggpubr)
library(kableExtra)
library(here)
source(here("scripts/compute_es.R"))
knitr::opts_chunk$set(
                      echo=F, warning=F, 
                      message=F, sanitize = T)

d <- readxl::read_xlsx(here("Effect size coding (online testing MA).xlsx"))

d <- d |>
  rowwise() |>
  mutate(short_cite = paste(str_c(str_split(authors, pattern = ",")[[1]][1], 
                                  collapse = " "), ":",
                            paste(str_split(main_title, pattern = " ")[[1]][1:2], 
                                  collapse = " ")))
d_calc <- d |>
  ungroup() |>
  mutate(idx = 1:n()) |>
  group_by(idx) |>
  nest() |>
  mutate(data = map(data, function (df) {
    es <- compute_es(participant_design = df$participant_design, 
                     x_1 = df$x_1, x_2 = df$x_2, 
                     SD_1 = df$SD_1, SD_2 = df$SD_2, n_1 = df$n_1, n_2 = df$n_2,
                     t = df$t, f = df$F, d = df$d, d_var = df$d_var)
    
    bind_cols(df,es)
  })) |>
  unnest(cols = c(data)) |>
  filter(!is.na(d_calc))
d_calc$Age <- d_calc$mean_age_1 - mean(d_calc$mean_age_1)
d_calc <- d_calc |>
  mutate(response_mode = case_when(response_mode == "non-looking" ~ "Verbal",
                                   response_mode == "looking" ~ "Looking")) %>% 
  mutate(expt_condition = case_when(expt_condition == "online" ~ "Online",
                                    expt_condition == "in-person" ~ "In-person")) %>% 
  mutate(method = case_when(method == "moderated" ~ "Moderated",
                            method == "unmoderated" ~ "Unmoderated"))

#d_calc
```

```{r coeffs, fig.pos="ht"}
main_mod <- rma.mv(yi = d_calc, 
                   V = d_var_calc,
                   mods = as.factor(expt_condition),
                   random = ~1 | as.factor(experiment_ID) + as.factor(same_infant), 
                   slab = short_cite, 
                   data = d_calc)
#summary(main_mod)

moderated_mod <- rma.mv(yi = d_calc, 
                        V = d_var_calc, 
                        mods = ~ as.factor(expt_condition) 
                        + as.factor(method),
                        random = ~1 | as.factor(experiment_ID) + 
                          as.factor(same_infant), 
                        slab = short_cite, 
                        data = d_calc)
#summary(moderated_mod)

looking_mod <- rma.mv(yi = d_calc, 
                      V = d_var_calc, 
                      mods = ~ as.factor(expt_condition) 
                      * as.factor(response_mode),
                      random = ~1 | as.factor(experiment_ID) + 
                        as.factor(same_infant), 
                      slab = short_cite, 
                      data = d_calc)
#summary(looking_mod)

age_mod <- rma.mv(yi = d_calc, 
                        V = d_var_calc, 
                        mods = ~ as.factor(expt_condition) 
                        * Age,
                        random = ~1 | as.factor(experiment_ID) + 
                          as.factor(same_infant), 
                        slab = short_cite, 
                        data = d_calc)
#summary(age_mod)

pull_coeffs <- function(mod){
  df <- tibble(Coefficient=rownames(mod$b), Estimate=mod$b |> round(2), `P-value`=mod$pval |> round(3), lower=mod$ci.lb |> round(2), upper=mod$ci.ub |> round(2)) |> 
    mutate(`95% CI`=str_c("[",lower,", ", upper,"]")) |> 
    select(Coefficient, Estimate, `95% CI`, `P-value`) |> 
    mutate(Coefficient=str_replace_all(Coefficient,coll("as.factor(expt_condition)"),"") |> 
             str_replace_all(coll("as.factor(response_mode)"),"") |>
             str_replace_all(coll("as.factor(method)"),"") |>
             str_replace_all(coll("mods"),"Online") |> 
             str_replace_all("intrcpt", "Intercept"))
  
  df
}

stats_text  <- function(model, row){
  str_c("Est=", model[row,2], ", 95% CI=", model[row,3],", p=", model[row,4])
}

pull_coeffs(main_mod) |> 
union(pull_coeffs(looking_mod)) |> 
  union(pull_coeffs(age_mod)) |> 
  union(pull_coeffs(moderated_mod)) |> 
  select(Coefficient, Estimate, `95% CI`, `P-value`) |> 
knitr::kable(caption = "Table of coefficients for the pre-registered models. The overall model is shown first, followed by the three models with moderators.", booktabs=T, format="latex", linesep="") |> 
  pack_rows("Overall", 1, 2) %>%
pack_rows("Looking v Verbal", 3, 6) %>%
pack_rows("Age", 7, 10) |>
  pack_rows("Moderated v Un-moderated", 11, 12) |> 
  kable_styling(latex_options = "hold_position")
```

```{r}
# heteorogeneity calculation for main_mod 
get_i2 <- function(res){
  W <- diag(1/res$vi)
X <- model.matrix(res)
P <- W - W %*% X %*% solve(t(X) %*% W %*% X) %*% t(X) %*% W
100 * sum(res$sigma2s) / (sum(res$sigma2s) + (res$k-res$p)/sum(diag(P)))

}


online_res <- rma.mv(d_calc ~ 1, 
                   V = d_var_calc,
                   random = ~1 | as.factor(experiment_ID) + as.factor(same_infant), 
                   slab = short_cite, 
                   data = d_calc %>% filter(expt_condition == "Online"))

in_person_res <- rma.mv(d_calc ~ 1, 
                   V = d_var_calc,
                   random = ~1 | as.factor(experiment_ID) + as.factor(same_infant), 
                   slab = short_cite, 
                   data = d_calc %>% filter(expt_condition == "In-person"))

#get_i2(online_res)
#get_i2(in_person_res)
#get_i2(main_mod)
#get_i2(moderated_mod)
#get_i2(looking_mod)
#get_i2(age_mod)
```

```{r forest, fig.env="figure", fig.pos = "h", fig.align = "center",out.width="100%", fig.width=10, fig.height=5, fig.cap = "Forest plots of studies, sorted by difference in SMD. Each row is one study (paper or pair of papers) and contains every effect size pair contributed by that study. Each dot represents the difference between a single in-person measure and a corresponding online measure. A: Difference in SMD by study and online method (moderated vs unmoderated). B: Difference in SMD by study and measurement type (looking vs verbal). C: Difference in SMD by study and mean participant age (months)." }

online <- d_calc |> ungroup() |> filter(expt_condition=="Online") |>
  mutate(n_2=replace_na(n_2,0), 
         mean_age_2=replace_na(mean_age_2,0),
         online_age=(n_1*mean_age_1+n_2*mean_age_2)/(n_1+n_2),
         online_count=n_1+n_2) |> 
  select(d, d_var, effect_size_group, response_mode, method, short_cite, online_age,online_count) |> 
  rename(d_online=d,d_var_online=d_var)

in_person <-  d_calc |> ungroup()|> filter(expt_condition=="In-person") |> 
   mutate(n_2=replace_na(n_2,0), 
         mean_age_2=replace_na(mean_age_2,0),
         inperson_age=(n_1*mean_age_1+n_2*mean_age_2)/(n_1+n_2),
         inperson_count=n_1+n_2) |> 
  select(d, d_var, effect_size_group, inperson_age, inperson_count) |> 
  rename(d_inperson=d,d_var_inperson=d_var)

pairs <- full_join(online, in_person) |> 
  mutate(d_diff=d_online-d_inperson,
         d_diff_var=d_var_online+d_var_inperson,
         age=(inperson_age*inperson_count+online_age*online_count)/
           (online_count+inperson_count))

age_plot = ggplot(pairs, aes(x = short_cite, y = d_diff, ymin=d_diff-d_diff_var,
                                 ymax=d_diff+d_diff_var)) + 
  geom_pointrange(aes(color=age),alpha = .5, position=position_dodge2(width=.5)) +
  coord_flip() +
  geom_hline(yintercept = 0, lty = 2) + 
  aes(x=reorder(short_cite,-d_diff, sum)) +
  ylab("SMD difference (online - in-person)") + 
  xlab("") +
    scale_color_continuous(name="Age (Months)") +
  theme_minimal() +
  theme(axis.text.y=element_blank(), axis.ticks.y=element_blank(), legend.position = "bottom")

looking_plot = ggplot(pairs, aes(x = short_cite, y = d_diff, ymin=d_diff-d_diff_var,
                                 ymax=d_diff+d_diff_var)) + 
  geom_pointrange(aes(color=response_mode),alpha = .5, position=position_dodge2(width=.5)) +
  coord_flip() +
  geom_hline(yintercept = 0, lty = 2) + 
  aes(x=reorder(short_cite,-d_diff, sum)) +
  ylab("SMD difference (online - in-person)") + 
  xlab("") +
scale_color_manual(values = wes_palette("Darjeeling1", n = 2), name = "Measure", labels = c("Looking", "Verbal")) +
  theme_minimal()+
    theme(axis.text.y=element_blank(), axis.ticks.y=element_blank(), legend.position = "bottom")

 
moderated_plot = ggplot(pairs, aes(x = short_cite, y = d_diff, ymin=d_diff-d_diff_var,
                                 ymax=d_diff+d_diff_var)) + 
  geom_pointrange(aes(color=method),alpha = .5, position=position_dodge2(width=.5)) +
  coord_flip() +
  geom_hline(yintercept = 0, lty = 2) + 
  aes(x=reorder(short_cite,-d_diff, sum)) +
  ylab("SMD difference (online - in-person)") + 
  xlab("") +
scale_color_manual(values = wes_palette("Cavalcanti1", n = 2), name = "Online method", labels = c("Moderated", "Unmoderated")) +
  theme_minimal()+
  theme(axis.text.y=element_blank(), axis.ticks.y=element_blank(), legend.position = "bottom")
 

#vanilla_plot
#age_plot
#looking_plot

#moderated_plot+looking_plot+age_plot

ggarrange(moderated_plot, looking_plot, age_plot,
          labels = c("A", "B", "C"),
          ncol = 3,
          nrow = 1)
```

## Planned Analysis

Overall, the meta-analysis revealed a small negative, non-significant effect of online study modality, `r pull_coeffs(main_mod) |> stats_text(2)`. Additionally, we did not find any significant effect of our preregistered moderators or any significant interactions between the moderators and study modality. See Table \@ref(tab:coeffs) for coefficient values. Figure \@ref(fig:forest) shows the effect size differences of experiments by moderators. 

Because our meta-analysis averaged across effects from very different paradigms (which could yield different effect sizes independent of the effect of testing modality), we expected substantial heterogeneity. Consistent with that expectation, all tests for residual heterogeneity were highly significant (all $p$s $< .0001$). Values of $\tau^2$ (the between-study variance in our meta-analysis) for all models were `r round(main_mod$tau2,2)` (primary model), `r round(moderated_mod$tau2,2)` (moderated vs. unmoderated model), `r round(looking_mod$tau2,2)` (looking-time model), and `r round(age_mod$tau2,2)` (age model), respectively, confirming the impression that these moderators did not reduce heterogeneity.

## Exploratory Analysis

```{r modmeans}
# library(magick)
combined_mod <- rma.mv(yi = d_calc, 
                        V = d_var_calc, 
                        mods = ~ as.factor(expt_condition)*as.factor(method)*as.factor(response_mode),
                        random = ~1 | as.factor(experiment_ID) + 
                          as.factor(same_infant), 
                        slab = short_cite, 
                        data = d_calc)
#summary(combined_mod)

combined <- d_calc %>% 
  group_by(experiment_ID, expt_condition, method, response_mode) %>% 
  summarise(mean_smd = mean(d))

grouped_calc <- combined |>
  group_by(as.factor(expt_condition), as.factor(method), as.factor(response_mode)) |> 
  summarise(count = n(),
            SMD = mean(mean_smd),
            se = sd(mean_smd)/sqrt(n()-1)) |> 
  mutate(lower=(SMD-1.96*se) |> round(3) |> as.character(),
         higher=(SMD+1.96*se) |> round(3) |> as.character(),
         `95% CI`=str_c("[",lower,", ",higher,"]")) |> 
  select(-se,-lower,-higher)
names(grouped_calc)[1] <- "Modality"
names(grouped_calc)[2] <- "Method"
names(grouped_calc)[3] <- "Measure"
names(grouped_calc)[4] <- "N (Effect-size Pairs)"

grouped_calc |>
  knitr::kable(caption="Mean SMD across studies by study modality, data-collection method, and type of dependent measure", booktabs=T, digits = 3, format="markdown")%>% kable_styling(full_width=T)
```

In addition to our multi-level meta-analysis, we examined which combinations of methods and measures tended to yield the strongest and weakest effect sizes relative to their in-person counterparts. We fit a meta-analytic model containing method, response mode, and modality as well as their two- and three-way interactions, with the same random effects structure as our previous model. We cannot draw any strong conclusions about these noisy estimates due to our relatively small sample size. That said, descriptively, unmoderated online studies with looking measures were estimated to have noticeably smaller effect sizes compared to both their moderated online and in-person counterparts (See Table \@ref(tab:modmeans)). In contrast, as estimated by this model, moderated online studies with looking and verbal measures as well as unmoderated online studies with verbal measures did not show such large differences from their in-person counterparts. 

```{r funnel, fig.env="figure", fig.pos = "h", fig.align = "center",out.width="80%", fig.width=6, fig.height=4, fig.cap = "Funnel plot of the differences in effect size between pairs of in-person and online studies. A positive observed outcome means the online study had a larger effect size. \\label{funnel}" }

# library(magick)
paired_mod <- rma.mv(d_diff ~ 1,  
       V = d_diff_var, 
       data = pairs)

funnel(paired_mod)
```

```{r}
funtest <- regtest(x=pairs$d_diff, vi=pairs$d_diff_var)
```

We also conducted an exploratory analysis of potential publication bias. It was unclear *a priori* how we might expect publication biases to manifest themselves, given that there is some possibility of notoriety for either showing *or* failing to show differences between online and in-person testing. In either case our hypothesized selection process operated on the *differences* in effect sizes between each online and in-lab pair of samples.

For each online and in-person pair on the same study, we calculated a standard mean difference in effect size between the two studies as well as the variance of this difference. The resulting funnel plot is shown in Figure \@ref(fig:funnel). As the difference in effect size increases, the variance should also increase; however, if asymmetries are observed in this relationship (e.g., a greater number of negative outcome values with low variance), effect sizes may not have been uniformly reported. According to Egger's regression test for funnel plot asymmetry, a common method for assessing publication bias in meta-analyses, this plot is asymmetric (p=`r papaja::printp(funtest$pval)`) and the estimated effect assuming no variance is `r funtest$est |> round(2)` [`r funtest$ci.lb |> round(2)`, `r funtest$ci.ub |> round(2)`]. This analysis suggests the possibility of publication bias favoring studies that have smaller effect sizes online compared to in-person, signaling that perhaps online studies may have relatively larger effect sizes on average compared to what has been reported. We interpret this conclusion with caution, however, noting the large width of the estimated CI and the relatively low power of Egger's test [@sterne2000]. 


