By aggregating across a growing literature of online studies, the current meta-analysis provides a birds-eye view of how developmental studies traditionally conducted in-person fare compared to closely matched counterparts conducted online. Our results suggest that overall, developmental studies conducted in-person do not yield significantly larger effect sizes compared to similar studies conducted online. Based on our analysis, the method of online data collection, type of dependent measure, and participant age did not have a significant impact either. Nonetheless, our lack of statistical precision, indicated by relatively wide confidence intervals, limits our ability to make strong conclusions about the effect of any of our moderators. Future analysis is needed to determine the moderating effect, if any, that these factors exercise on the outcome of developmental studies conducted online.

It is also important to consider additional factors that could influence these results or the way we interpret them. Chiefly, the current analysis is quite coarse-grained and considers one particular dichotomy within study modality: in-person vs online. Yet, there are many ways that developmental studies can be further subdivided. For example, studies are conducted both in quiet spaces (e.g., in lab, at home) and loud spaces (e.g., parks, museums). Therefore, online studies might out- or underperform studies conducted in particular in-person locations. Our moderators are also correspondingly course-grained, particularly dependent measure (looking vs verbal). Because our small sample size renders our analysis underpowered to detect weaker effects of moderators, the current results and their interpretation are subject to change as online methods improve and comparisons to in-person studies are better understood.

Nonetheless, unmoderated studies with looking measures noticeably had the smallest effect sizes relative to their in-person counterparts. This could reflect the relative difficulty of both collecting and coding looking data online using participants' own webcams. However, smaller effect sizes online could instead reflect genuinely smaller effect sizes of the in-person results rather than a lack of online studies' sensitivity. Developmental research has suffered from many failures to replicate in the past, especially studies with infants [e.g., @davis2019overview], and many of the online studies in our sample were conducted after their in-person counterparts, sometimes years later. Therefore, it is possible that smaller online effect sizes simply represent a more accurate estimation of the true (smaller) effect rather than an effect of study modaility per se.

Although developmental researchers have had decades of experience designing and running experiments in-person, most have only had a few years or less of experience developing online studies. Thus, our meta-analysis might also underestimate the potential of online studies due to researcher and experimenter inexperience. Over the next several years, as developmental researchers develop expertise and experience with online studies, online studies might become more accurate at capturing cognitive constructs for any number of reasons, including better experimenter-participant interactions, better stimulus design [see @chuey2021moderated], and more accurate methods of measurements [i.e., automatic looking time measures, see @erel2022icatcher]. Relatedly, as new methods are developed and adapted for online experiments, researchers should not take the current findings as a blanket declaration that all online studies produce comparable results to their in-person counterparts; some might underperform, while others might outperform. Nonetheless, the current results suggest that across currently employed developmental methodologies, studies conducted with children online are generally comparable to those conducted in-person.

The composition of our sample might also bias our results. To match online and in-person methods as closely as possible, we only considered direct online replications for the current meta-analysis. While this approach ensures that data were collected online and in-person using similar methods and procedures, it limits our sample size and may bias our sample. For example, perhaps researchers disproportionately choose to conduct online replications of strong or well-established effects rather than replicate more subtle, weaker effects. Nonetheless, our analysis found that if publication bias exists, it likely favors stronger in-person effect sizes or non-replications among the studies we sampled. We also included an open call for unpublished data in an attempt to limit the file drawer problem [see @rosenthal1979file].

# Conclusion 

Our meta-analysis found that, across closely matched developmental studies conducted in-person and online, study modality did not have a significant effect on the size of the main effect of interest. Although we also found no significant effect of how the online studies were conducted (moderated vs unmoderated), dependent measures (looking vs verbal), or participant age, our lack of statistical precision limit our ability to make strong conclusions about their relationship to online replicability. In particular, we found that unmoderated looking studies conducted online had noticeably smaller effect sizes relative to their in-person counterparts; however, it is unclear whether this is reflective of the limits of online methodology in-person or a lack of replicability for these kinds of studies more generally. Therefore, as additional online replications are published, experimenters should also take their findings into account when considering the appropriate modality to conduct their particular flavor of developmental research. Nonetheless, the general similarity in outcomes for in-person and online studies with children paint an optimistic picture for online developmental research more broadly going forward. 

