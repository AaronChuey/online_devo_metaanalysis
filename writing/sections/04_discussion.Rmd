By aggregating across a growing literature of online studies, the current meta-analysis provides a birds-eye view of how developmental studies traditionally conducted in-person fare compared to closely matched counterparts conducted online. Our results suggest that overall, developmental studies conducted in-person do not yield significantly larger effect sizes compared to similar studies conducted online. Based on our analysis, the method of online data collection, type of dependent measure, and participant age did not have a significant impact either. Nonetheless, our lack of statistical precision, indicated by relatively wide confidence intervals, limits our ability to make strong conclusions about the effect of any of our moderators. Future analysis is needed to determine the moderating effect, if any, that these factors exercise on the outcome of developmental studies conducted online.

It is also important to consider additional factors that could influence these results or the way we interpret them. Chiefly, the current analysis is quite coarse-grained and considers one particular dichotomy within study modality: in-person vs online. Yet, there are many ways that developmental studies can be further subdivided. For example, studies are conducted both in quiet spaces (e.g., in lab, at home) and loud spaces (e.g., parks, museums). Therefore, online studies might out- or underperform studies conducted in particular in-person locations. Our moderators are also correspondingly course-grained, particularly dependent measure (looking vs verbal). Unmoderated studies with looking measures had the smallest effect sizes relative to their in-person counterparts. However, smaller effect sizes online could reflect true non-replications of the in-person results rather than a lack of online studies' sensitivity. Because our small sample size renders our analysis underpowered to detect weaker effects of moderators, the current results and their interpretation are subject to change as online methods improve and comparisons to in-person studies are better understood.

Although developmental researchers have had decades of experience designing and running experiments in-person, most have only had a few years or less of experience developing online studies. Thus, our meta-analysis might underestimate the potential of online studies due to researcher and experimenter inexperience. Over the next several years, as developmental researchers develop expertise and experience with online studies, effect sizes might increase for any number of reasons, including better experimenter-participant interactions, better stimulus design, and more accurate methods of measurements [i.e., automatic looking time measures, see @erel2022icatcher]. Relatedly, as new methods are developed and adapted for online experiments, researchers should not take the current findings as a blanket declaration that all online studies produce comparable results to their in-person counterparts; some might underperform, while others might outperform. Nonetheless, the current results suggest that across currently employed developmental methodologies, studies conducted with children online are generally comparable to those conducted in-person.

The composition of our sample might also bias our results. To match online and in-person methods as closely as possible, we only considered direct online replications for the current meta-analysis. While this approach ensures that data were collected online and in-person using similar methods and procedures, it limits our sample size and may bias our sample. For example, perhaps researchers disproportionately choose to conduct online replications of strong or well-established effects rather than replicate more subtle, weaker effects. Nonetheless, our analysis found no significant publication bias in terms of favoring stronger online effect sizes or non-replications among the studies we sampled. We also included an open call for unpublished data in an attempt to limit the file drawer problem [see @rosenthal1979file]. Of the published and unpublished online replications that were available to include in our sample, we found similar effect sizes online compared to in-person. 

# Conclusion 

Our meta-analysis found that, across closely matched developmental studies conducted in-person and online, study modality did not have a significant effect on the size of the main effect of interest. Although we also found no significant effect of how the online studies were conducted (moderated vs unmoderated), dependent measures (looking vs verbal), or participant age, our lack of statistical precision limit our ability to make strong conclusions about their relationship to online replicability. In particular, we found that unmoderated looking studies conducted online had noticeably smaller effect sizes relative to their in-person counterparts; however, it is unclear whether this is reflective of the limits of online methodology in-person or a lack of replicability for these kinds of studies more generally. Therefore, as additional online replications are published, experimenters should also take their findings into account when considering the appropriate modality to conduct their particular flavor of developmental research. Nonetheless, the general similarity in outcomes for in-person and online studies with children paint an optimistic picture for online developmental research more broadly going forward. 

