```{r prisma, fig.cap = "PRISMA plot detailing our study screening process; numerical values represent the number of papers at each stage of the systematic search."}

#source(here("scripts/prisma_diagram.R"))


#devtools::install_github("prisma-flowdiagram/PRISMA2020")
#library(PRISMA2020) this is too restrictive, so we edited the source code instead
library(tidyverse)
library(here)
library(googledrive)

# Read in Data 
#d <- read_sheet("https://docs.google.com/spreadsheets/d/1ctTyXRj0Sd037P0zTd-eulFQjrWEbuOx-c-n5VAcwr0/edit#gid=0")
#Veronica was having Oath problems
#f <- googledrive::as_dribble("https://docs.google.com/spreadsheets/d/1ctTyXRj0Sd037P0zTd-eulFQjrWEbuOx-c-n5VAcwr0/edit#gid=0")
#googledrive::drive_download(f, path=here("Online Testing Paper Decision Spreadsheet.xlsx"), overwrite=T)
#g <- googledrive::as_dribble("https://docs.google.com/spreadsheets/d/1OIrWIsqjuXVBQYdu90AjJ9b--15GfwTjwvqtOhq11oo/edit#gid=0")
#googledrive::drive_download(g, path=here("Effect size coding (online testing MA).xlsx"), overwrite=T)

source(here("scripts/utils.R"))
source(here("scripts/globals.R"))
source(here("scripts/PRISMA_flowdiagram.R"))


d <- readxl::read_xlsx(here("Online Testing Paper Decision Spreadsheet.xlsx"))

per_expt <- readxl::read_xlsx(here("Effect size coding (online testing MA).xlsx"))

foo <- read_csv(here("data/PRISMA.csv")) 

d_found <- d %>% group_by(Source) |> tally()

d_database <- d_found |> filter(Source %in% c("Lookit1", 
                    "Lookit2", "Rhodes", "Sheskin&Keil")) |> 
  mutate(Source=case_when(
    Source=="Lookit1" ~ "Lookit Part 1 ",
    Source=="Lookit2" ~ "Lookit Part 2 ",
    Source=="Sheskin&Keil" ~ "The Child Lab ",
    Source=="Rhodes" ~ "Pandas "
  )) |> #hilariously can't put commas because that's used as the splitting character in the prisma diagram maker...so can't put real citations for these
  summarize(blah = str_c(Source,n,sep=" , ", collapse=" ; ")) 
  
foo$n[6] <- d_database[[1]]
foo$boxtext[5] <- "Forward citation search"
foo$n[5] <- d |> filter(Source %in% c( "Lookit1", 
                    "Lookit2", "Rhodes", "Sheskin&Keil")) |> nrow()

d_other <- d_found |> filter(!Source %in% c("Lookit1", 
                    "Lookit2", "Rhodes", "Sheskin&Keil")) |> 
  mutate(Source=case_when(
    Source=="ICIS" ~ "ICIS listserv",
    Source=="cogdevsoc" ~ "CDS listserv",
    Source=="Frontiers" ~ "Frontiers Special Issue articles",
    Source=="Zaadnoordijk&Cusack" ~ "List from Zaadnoordijk & Cusack",
    Source=="etc" ~ "personal communication",
    T ~ Source
  )) |> 
  summarize(blah = str_c(Source,n,sep=" , ", collapse=" ; "))

foo$n[8] <- d_other[[1]]
foo$boxtext[7] <- "Other sources"
foo$n[7] <- d |> filter(!Source %in% c( "Lookit1", 
                    "Lookit2", "Rhodes", "Sheskin&Keil")) |> nrow()

# identify total duplicates records 
d_duplicate <- d %>% 
  mutate(duplicate = case_when(
    Title_screening_decision == "duplicate" ~ "yes", 
    TRUE ~ "no"
  )) %>% 
  group_by(duplicate) %>% 
  count()

foo$n[13] <- d_duplicate[2,2]
foo$n[14]=NA
foo$n[15]=NA

d_record_screened <- d %>% 
  filter(Title_screening_decision != "duplicate")

foo$n[16] <- d_record_screened |> nrow()

d_record_screened_summary <- d %>% 
  filter(Title_screening_decision != "duplicate") %>% 
  group_by(Title_screening_decision,Abstract_screening_decision) %>% 
  count()

foo$n[17] <- d_record_screened_summary[1,3]+d_record_screened_summary[2,3] #counts both the excluded at title and at abstract stages
foo$n[18] <- d_record_screened_summary[3,3]
foo$n[22] <- d_record_screened_summary[3,3]


d_full_text <- d %>% 
  filter(Abstract_screening_decision == "yes") %>% 
  filter(Eligibility_decision=="no") |> 
  group_by(Criterion_for_exclusion) %>% 
  mutate(Criterion_for_exclusion=case_when(
    Criterion_for_exclusion=="Participants" ~"Participant mean age > 6 years",
    Criterion_for_exclusion=="Comparison" ~ "No in-person sample for comparison",
    Criterion_for_exclusion=="Comparability" ~ "In-person and online samples not comparable",
    T ~ Criterion_for_exclusion
  )) |> 
  count() |> ungroup() |> summarize(blah = str_c(Criterion_for_exclusion,n,sep=" , ", collapse=" ; "))

foo$n[23] <- d_full_text[[1]]


foo$n[27] <- d |> filter(Eligibility_decision=="yes") |> nrow()

#per_expt |> distinct(paper_ID) |> nrow()
#per_expt |> nrow()
foo$n[26] <- per_expt |> distinct(expt_num) |> nrow()
foo$boxtext[26] <- "Experimental measures included in review"
foo$boxtext[27] <- "Reports of included studies"
 PRISMA_data(foo) |> PRISMA_flowdiagram(previous=F, other=F, detail_databases = T, detail_registers = T, fontsize=12,side_boxes=F)

```

We conducted a literature search following the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) procedure [@page2021prisma]; see Figure \@ref(fig:prisma). For each set of studies determined to be an online replication, we calculated the effect size(s) and associated variance for the main effect of interest. We then conducted a series of random-effects multilevel meta-regressions to estimate the effect of online data collection, as well as three possible moderators (online study method, type of dependent measure, and participant age). Our preregistered data selection, coding, and analysis plan can be found at [https://osf.io/hjbxf](). The list of papers included in this meta-analysis is shown in Table \@ref(tab:list).

## Literature Search 

Our goal was to find as many published and unpublished online replications of developmental studies as possible. However, because there is no common nomenclature for online replications and the studies themselves cover a wide range of research questions and methodologies, searching via specific terms or keywords was difficult and produced many irrelevant papers; as a result, we could not conduct a completely systematic review. Instead, we preregistered a forward citation search strategy based on key papers on online developmental research. We used the papers that conducted initial validation of popular online testing platforms as our seeds, including Lookit [@scott2017lookit; @scott2017lookitB], The Child Lab [@sheskin2018thechildlab], and Pandas [@rhodes2020advancing]. Any paper that cited at least one of these papers was considered for inclusion in our meta-analysis. We also considered all papers published in the Frontiers in Psychology Special Issue: Empirical Research at a Distance: New Methods for Developmental Science, which largely focused on online developmental studies and replications. Additionally, we were were pointed to [@zaadnoordijk2022online] which contained a list of online replication papers, although this yielded few additional replications. Finally, we posted a call for contributions to the Cognitive Development Society (CDS) and International Congress of Infant Studies (ICIS) listservs, two popular emailing lists frequented by developmental researchers. This call yielded several publications our initial search strategy missed, as well as six unpublished but complete online replications.

We preregistered several eligibility criteria to filter articles from our search: 

1. The study must be experimental, where participants complete a task with a stimulus. This criterion precludes surveys or purely observational measures. 

2. The studies must report two groups of children, one tested online and another tested in-person. Although the online sample must be collected by the researchers reporting the results, the in-person sample could either be collected at the same time or referenced from an existing publication. 

3. The mean age of the sample should be under six years. This criterion limits the studies to those conducted on relatively younger children for whom online data collection methods have not been traditionally employed. 

4. All data reported or referred to must contain codable effect sizes. Verbal comparison alone between an online or in-person study or a qualitative description of results is not enough to determine the precise effect size of interest. 

5. Data collection for both the in-person and online sample must be complete; any incomplete or partial samples were not considered.

6. The online and in-person methods must be directly comparable. Some alteration to the study methods is expected when adapting an in-person study to be run online (e.g., having children refer to objects by color instead of pointing). However, we excluded any studies whose methodologies altered the nature of the task or the conclusions that could be drawn from them (e.g., changing the identity of a hidden object instead of its location in a false belief task).

```{r}
source(here("scripts/compute_es.R"))

knitr::opts_chunk$set(
                      echo=F, warning=F, 
                      message=F, sanitize = T)

#d <- read_sheet("https://docs.google.com/spreadsheets/d/1OIrWIsqjuXVBQYdu90AjJ9b--15GfwTjwvqtOhq11oo/edit#gid=0")

d <- readxl::read_xlsx(here("Effect size coding (online testing MA).xlsx"))

ages <- d |> select(short_cite,expt_condition, mean_age_1, mean_age_2, n_1, n_2, response_mode, method) |> 
  mutate(avg_age=ifelse(is.na(n_2), mean_age_1, (mean_age_1*n_1+mean_age_2*n_2)/(n_1+n_2)),
         n=ifelse(is.na(n_2),n_1, n_1+n_2)) |> 
  group_by(short_cite) |> 
  summarize(avg_age=weighted.mean(avg_age,n),
            n_studies=n())

mod <- d |> select(short_cite, response_mode) |> unique() |> 
  group_by(short_cite) |> 
  mutate(val=1) |> 
  pivot_wider(names_from=c(response_mode), values_from=val, values_fill = 0) |> 
  mutate(method=case_when(
    `non-looking`==1&looking==1 ~ "Both",
    `non-looking`==1 ~ "Verb",
    looking==1 ~ "Look"
  )) |> select(short_cite, method)

condition <- d |> filter(expt_condition=="online") |>  select(short_cite, method) |> unique() |> 
  group_by(short_cite) |> 
  mutate(val=1) |> 
  pivot_wider(names_from=c(method), values_from=val, values_fill = 0) |> 
  mutate(moderation=case_when(
    `unmoderated`==1&moderated==1 ~ "Both",
    `unmoderated`==1 ~ "Unmod",
    moderated==1 ~ "Mod"
  )) |> select(short_cite, moderation)

online <- d|> ungroup() |> filter(expt_condition=="online") |>
  mutate(n_2=replace_na(n_2,0), 
         mean_age_2=replace_na(mean_age_2,0),
         online_age=(n_1*mean_age_1+n_2*mean_age_2)/(n_1+n_2),
         online_count=n_1+n_2) |> 
  select(d, d_var, effect_size_group, response_mode, method, short_cite, online_age,online_count) |> 
  rename(d_online=d,d_var_online=d_var)

in_person <-  d |> ungroup()|> filter(expt_condition=="in-person") |> 
   mutate(n_2=replace_na(n_2,0), 
         mean_age_2=replace_na(mean_age_2,0),
         inperson_age=(n_1*mean_age_1+n_2*mean_age_2)/(n_1+n_2),
         inperson_count=n_1+n_2) |> 
  select(d, d_var, effect_size_group, inperson_age, inperson_count) |> 
  rename(d_inperson=d,d_var_inperson=d_var)

pairs <- full_join(online, in_person) |> 
  mutate(d_diff=d_online-d_inperson,
         d_diff_var=d_var_online+d_var_inperson,
         age=(inperson_age*inperson_count+online_age*online_count)/
           (online_count+inperson_count)) |> 
  group_by(short_cite) |> summarize(pairs=n())

studies <- ages |> left_join(pairs) |> left_join(mod) |> left_join(condition) |> select(-n_studies) |> mutate(avg_age=round(avg_age)) |> ungroup()
```

```{r list}
d |> filter(expt_condition=="in-person") |> select(main_title, `comparison-title`, short_cite, comparison_cite) |> unique() |> left_join(studies) |> 
  mutate(comparison_cite=ifelse(short_cite=="scott2017lookitB","teglas2007intuitions and @pasquini2007preschoolers", comparison_cite)) |> #this is bad practice, but we really do need to fix just this one row!
  mutate(short_cite=str_c("@", short_cite), comparison_cite=ifelse(is.na(comparison_cite),"",str_c("compared to @",comparison_cite))) |> 
  mutate(`Paper`=str_c(short_cite," ",comparison_cite)) |> 
  
  select(`Paper`, `Pairs`=pairs, `Look`=method, `Mod`=moderation, `Age`=avg_age) |> 
   unique() |> 
  arrange(Age) |> 
  knitr::kable(caption = "Papers used in this meta-analysis, ordered by average participant age (in months). Some papers contained both online and in-person results, others contained online replications compared to previous in-person papers. Pairs refers to the number of paired online and in-person effect sizes contributed by each paper (set). Look is whether the studies use looking, verbal, or both types of dependent measures. Mod is whether the online studies were moderated, unmoderated, or both. ", booktabs=T, format="markdown", linesep="")
```

## Data Entry 

All papers (233) yielded by our search procedure went through three rounds of evaluation to determine if they met our inclusion criteria. First, we screened the titles of the papers to determine whether they might include an online experiment. Those that clearly did not meet one or more of our inclusion criteria were excluded from further evaluation. Next, we performed a similar evaluation based on the papers’ abstracts, before a final round based on the article as a whole. All remaining papers were entered into a spreadsheet that coded the necessary information for calculating the size of the main effect(s) of interest and their associated variance (sample size, group means and standard deviation, and t and F statistics when applicable), as well as our preregistered moderators (study modality, data collection method, dependent measure, and participant age). 

If a paper reported an effect size as Cohen's d (referred to below as standardized mean difference, SMD), we coded it directly. Otherwise, we calculated the individual effect sizes for each main effect and each study (online and in-person) via reported means and standard deviations, t-statistic, or directly from the data if it was available using analysis scripts adapted from Metalab [e.g., @bergmann2018], a repository of meta-analyses in early language and cognitive development. If the main comparison was to chance performance, we first calculated log odds and then converted the effect size to cohen's d via the compute.es package in R (Del Re & Del Re, 2012). If a given study had multiple dependent measures or central hypotheses, we calculated an effect size and associated variance for each.

## Analytic Approach 

To determine whether study modality (online or in-person) moderated the size of the main effect of interest for each set of studies, we performed a preregistered random-effects multilevel meta-regression using the metafor package in R [@viechtbauer2010conducting]. The regression predicted individual study effect size (SMD) with study modality as a fixed effect, modeling individual experimental effect sizes with the coefficient of interest being the study modality predictor (online vs. in-person). As discussed above, we did not predict a direction of effect for the study modality predictor.

Our approach focused on the study modality moderator, rather than computing an online-offline difference score for each study and estimating the size of that difference directly. Although at a first glance this approach may seem simpler, many papers are heterogeneous and contain multiple online studies for a single given offline study, or multiple measures within the same study. In these cases, the appropriate difference was not always clear. For this reason, we chose to enter all study effects into the meta-regression and use the study modality moderator to estimate systematic modality effects.

To ensure that differences in the total number of effect sizes across studies did not bias our analysis by overweighting studies with more measurements, we utilized a four-level nested random effects structure, assigning random intercepts to each hierarchical level. Individual papers (level 1) contain sets of experiment pairs (level 2) that involve one or more samples of participants (level 3) from which individual effect sizes are derived (level 4). In effect, level 1 captures variation between different papers or pairs of papers included in our metanalysis, level 2 captures variation between particular experiments within those papers (e.g., modeling the dependency between multiple measurements reported from a single experiment), level 3 captures variation between different groups of participants (e.g., modeling the dependency between effect sizes from participants who completed a battery of tasks with multiple effects of interest^[There are three instances in our sample where the same group of participants participated in multiple different experiments, breaking this dependency. However, because this was a rare exception, we retained four-level nested structure.]), and level 4 captures variation between different effect size measures within a single sample (e.g., modeling the dependency between multiple effect sizes yielded by a particular sample from a particular experiment). It is important to note that we had originally preregistered a simpler random effects structure that assigned random intercepts by experiment and sample independently. However, this strategy incorrectly assumed these factors were independent, so we modified our models as described.

To determine the effect of additional moderators -- online study method (moderated vs unmoderated), dependent measure (looking vs verbal), and participant age - we conducted three additional multilevel meta-regressions each with an additional fixed effect plus the corresponding interaction with study modality. All analysis scripts were preregistered, and the code is available at [https://osf.io/up6qn]().